---
title: "R_club"
author: "Ruijuan Li"
date: "12/6/2017"
output: html_document
---

### Lab

12-06-2017 
### Linear regression 
```{r}
library(MASS)
library(ISLR)

# simple linear regression 
# fix(Boston)
names(Boston)
?Boston

lm.fit = lm(medv ~ lstat, data = Boston)
lm.fit
summary(lm.fit)

names(lm.fit)
coef(lm.fit)

confint(lm.fit)

predict(lm.fit,data.frame(lstat=c(5,10,15)), interval ="confidence")
predict(lm.fit,data.frame(lstat=c(5,10,15)), interval ="prediction") # difference between these two 

attach(Boston)
plot(lstat, medv)
abline(lm.fit)

par(mfrow=c(2,2))
plot(lm.fit)

plot(predict(lm.fit), residuals(lm.fit)) # residule 
plot(predict(lm.fit), rstudent(lm.fit)) # studendized residual, what is this? 

plot(hatvalues (lm.fit)) # what???? 
which.max(hatvalues (lm.fit))
# prediction interval VS confidence interval 
```

### 12-13-2017 Multiple linear regression 
Q I have when reading 
1) for multiple regression, what if there is repulsive interaction effect, then those predictor will not be identified, compare to scanone  CIM  and scantwo 
2) genomic prediction remove highly correlated markers similar to here
3) F-test: The test statistic in an F-test is the ratio of two scaled sums of squares reflecting different sources of variability. These sums of squares are constructed so that the statistic tends to be greater when the null hypothesis is not true. Anova() 
4) I like the confidence interval and prediction interval explanation part 
```{r}
library(MASS)
library(ISLR)
lm.fit <- lm(medv ~ lstat + age, data = Boston) 
summary(lm.fit)

# regression using all variable 
lm.fit <- lm(medv ~ ., data = Boston)
summary(lm.fit)

library(car)
vif(lm.fit) # don't understand ... look for colinearity problem 
?vif # variance inflation factor 

# regression using all but one variable 
lm.fit1 <- lm(medv ~ .-age, data = Boston)
summary(lm.fit1)

# or use update to update the variable 
lm.fit1 <- update(lm.fit, ~.-age) 
```

### 12-20-2017 interaction term, non-linear transformation of the predictors 
```{r}
attach(Boston)
summary(lm(medv ~ lstat*age,data=Boston))
lm.fit2=lm(medv ~ lstat+I(lstat^2)) 
summary(lm.fit2)

lm.fit=lm(medv~lstat)
anova(lm.fit ,lm.fit2) # variance explained by the two models, are they significantly different? F-test 
par(mfrow=c(2,2))
plot(lm.fit2)

lm.fit5=lm(medv~poly(lstat ,5)) 
summary(lm.fit5)
summary(lm(medv~log(rm),data=Boston))

# fix(Carseats)
names(Carseats)  

lm.fit=lm(Sales ~ .+Income:Advertising+Price:Age,data=Carseats)
summary(lm.fit)

attach(Carseats)
contrasts(ShelveLoc) 
```

### 01-16-2018 logistic regression 
```{r}
library(ISLR)
names(Smarket)
summary(Smarket)

cor(Smarket[,-9])
attach(Smarket)
plot(Volume)

glm.fits=glm(Direction ~ Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=Smarket ,family=binomial)
summary(glm.fits)

coef(glm.fits)
summary(glm.fits)$coef

glm.probs=predict(glm.fits,type="response")
glm.probs[1:10]
length(glm.probs)

contrasts (Direction )

glm.pred=rep("Down",1250)
glm.pred[glm.probs >.5]="Up"

glm.pred

table(glm.pred,Direction)
(507+145) /1250 

mean(glm.pred==Direction)

train =( Year <2005)
Smarket.2005= Smarket [! train ,]
dim(Smarket.2005) # 252 
train %>% sum()
Direction.2005=Direction[!train]

glm.fits=glm(Direction ~ Lag1+Lag2+Lag3+Lag4+Lag5+Volume , data=Smarket ,family=binomial,subset=train)

glm.probs=predict(glm.fits,Smarket.2005,type="response")

glm.pred=rep("Down",252)
glm.pred[glm.probs >.5]="Up"
table(glm.pred,Direction.2005)

mean(glm.pred==Direction.2005)
mean(glm.pred!=Direction.2005)

(97+34)/252

glm.fits=glm(Direction~Lag1+Lag2,data=Smarket ,family=binomial, subset=train)
glm.probs=predict(glm.fits,Smarket.2005,type="response")
glm.pred=rep("Down",252)
glm.pred[glm.probs >.5]="Up"
table(glm.pred,Direction.2005)

mean(glm.pred==Direction.2005)
106/(106+76)



```

### 01-23-2018 linear discriminant analysis 
```{r}
# LDA: linear discriminant analysis 
library(MASS)
library(ISLR)
attach(Smarket)
train =Smarket$Year < 2005
Smarket.2005= Smarket [!train,]
Direction.2005=Direction[!train]

lda.fit=lda(Direction ~ Lag1+Lag2,data=Smarket, subset=train)
lda.fit

lda.pred=predict(lda.fit, Smarket.2005)
names(lda.pred)

lda.class=lda.pred$class
table(lda.class ,Direction.2005)

mean(lda.class==Direction.2005)
sum(lda.pred$posterior[,1]>=.5) 
sum(lda.pred$posterior[,1]<.5) 

lda.pred$posterior[1:20,1] 
lda.class[1:20]

sum(lda.pred$posterior[,1]>.9)  

# pi(k) = Pr(Y = k): prior probaility 
# fk(x): probability of y=k for given X=x. 

# random cross validataion: to avoid samples which have abnormal values fall into the same group 
```


