---
title: "R_club"
author: "Ruijuan Li"
date: "12/6/2017"
output: html_document
---

### Lab

12-06-2017 
### Linear regression 
```{r}
library(MASS)
library(ISLR)

# simple linear regression 
# fix(Boston)
names(Boston)
?Boston

lm.fit = lm(medv ~ lstat, data = Boston)
lm.fit
summary(lm.fit)

names(lm.fit)
coef(lm.fit)

confint(lm.fit)

predict(lm.fit,data.frame(lstat=c(5,10,15)), interval ="confidence")
predict(lm.fit,data.frame(lstat=c(5,10,15)), interval ="prediction") # difference between these two 

attach(Boston)
plot(lstat, medv)
abline(lm.fit)

par(mfrow=c(2,2))
plot(lm.fit)

plot(predict(lm.fit), residuals(lm.fit)) # residule 
plot(predict(lm.fit), rstudent(lm.fit)) # studendized residual, what is this? 

plot(hatvalues (lm.fit)) # what???? 
which.max(hatvalues (lm.fit))
# prediction interval VS confidence interval 
```

### 12-13-2017 Multiple linear regression 
Q I have when reading 
1) for multiple regression, what if there is repulsive interaction effect, then those predictor will not be identified, compare to scanone  CIM  and scantwo 
2) genomic prediction remove highly correlated markers similar to here
3) F-test: The test statistic in an F-test is the ratio of two scaled sums of squares reflecting different sources of variability. These sums of squares are constructed so that the statistic tends to be greater when the null hypothesis is not true. Anova() 
4) I like the confidence interval and prediction interval explanation part 
```{r}
library(MASS)
library(ISLR)
lm.fit <- lm(medv ~ lstat + age, data = Boston) 
summary(lm.fit)

# regression using all variable 
lm.fit <- lm(medv ~ ., data = Boston)
summary(lm.fit)

library(car)
vif(lm.fit) # don't understand ... look for colinearity problem 
?vif # variance inflation factor 

# regression using all but one variable 
lm.fit1 <- lm(medv ~ .-age, data = Boston)
summary(lm.fit1)

# or use update to update the variable 
lm.fit1 <- update(lm.fit, ~.-age) 
```

### 12-20-2017 interaction term, non-linear transformation of the predictors 
```{r}
attach(Boston)
summary(lm(medv ~ lstat*age,data=Boston))
lm.fit2=lm(medv ~ lstat+I(lstat^2)) 
summary(lm.fit2)

lm.fit=lm(medv~lstat)
anova(lm.fit ,lm.fit2) # variance explained by the two models, are they significantly different? F-test 
par(mfrow=c(2,2))
plot(lm.fit2)

lm.fit5=lm(medv~poly(lstat ,5)) 
summary(lm.fit5)
summary(lm(medv~log(rm),data=Boston))

# fix(Carseats)
names(Carseats)  

lm.fit=lm(Sales ~ .+Income:Advertising+Price:Age,data=Carseats)
summary(lm.fit)

attach(Carseats)
contrasts(ShelveLoc) 
```

### 01-16-2018 logistic regression 
```{r}
library(ISLR)
names(Smarket)
summary(Smarket)

cor(Smarket[,-9])
attach(Smarket)
plot(Volume)

glm.fits=glm(Direction ~ Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=Smarket ,family=binomial)
summary(glm.fits)

coef(glm.fits)
summary(glm.fits)$coef

glm.probs=predict(glm.fits,type="response")
glm.probs[1:10]
length(glm.probs)

contrasts (Direction )

glm.pred=rep("Down",1250)
glm.pred[glm.probs >.5]="Up"

glm.pred

table(glm.pred,Direction)
(507+145) /1250 

mean(glm.pred==Direction)

train =( Year <2005)
Smarket.2005= Smarket [! train ,]
dim(Smarket.2005) # 252 
train %>% sum()
Direction.2005=Direction[!train]

glm.fits=glm(Direction ~ Lag1+Lag2+Lag3+Lag4+Lag5+Volume , data=Smarket ,family=binomial,subset=train)

glm.probs=predict(glm.fits,Smarket.2005,type="response")

glm.pred=rep("Down",252)
glm.pred[glm.probs >.5]="Up"
table(glm.pred,Direction.2005)

mean(glm.pred==Direction.2005)
mean(glm.pred!=Direction.2005)

(97+34)/252

glm.fits=glm(Direction~Lag1+Lag2,data=Smarket ,family=binomial, subset=train)
glm.probs=predict(glm.fits,Smarket.2005,type="response")
glm.pred=rep("Down",252)
glm.pred[glm.probs >.5]="Up"
table(glm.pred,Direction.2005)

mean(glm.pred==Direction.2005)
106/(106+76)
```

### 01-23-2018 linear discriminant analysis 
```{r}
# LDA: linear discriminant analysis 
library(MASS)
library(ISLR)
attach(Smarket)
train =Smarket$Year < 2005
Smarket.2005= Smarket [!train,]
Direction.2005=Direction[!train]

lda.fit=lda(Direction ~ Lag1+Lag2,data=Smarket, subset=train)
lda.fit

lda.pred=predict(lda.fit, Smarket.2005)
names(lda.pred)

lda.class=lda.pred$class
table(lda.class ,Direction.2005)

mean(lda.class==Direction.2005)
sum(lda.pred$posterior[,1]>=.5) 
sum(lda.pred$posterior[,1]<.5) 

lda.pred$posterior[1:20,1] 
lda.class[1:20]

sum(lda.pred$posterior[,1]>.9)  

# pi(k) = Pr(Y = k): prior probaility 
# fk(x): probability of y=k for given X=x. 

# random cross validataion: to avoid samples which have abnormal values fall into the same group
```

### 01-30-2018 QDA & KNN 
```{r}
# Qs on the text: page 150, LDA vs QDA vs KNN 
# t distribution on page 153
# correlation between predictors... ??? don't quite get it...  

library(MASS) 
library(ISLR)
attach(Smarket)
train =Smarket$Year < 2005
Smarket.2005= Smarket [!train,]
Direction.2005=Direction[!train]

### QDA 
qda.fit=qda(Direction ~ Lag1+Lag2,data=Smarket ,subset=train)
qda.fit # don't understand the summary result 

qda.class=predict(qda.fit,Smarket.2005)$class 
table(qda.class ,Direction.2005)

mean(qda.class==Direction.2005)

unique(Smarket$Year)

### KNN 
library(class)
Lag1
Lag2
train.X=cbind(Lag1 ,Lag2)[train ,] # the two P, Lag1 and Lag2, traning set 
test.X=cbind(Lag1,Lag2)[!train,] # test set
train.Direction =Direction [train] # direction for training set 
train.Direction

set.seed(1) # why set.seed() ?
knn.pred=knn(train.X,test.X,train.Direction ,k=1)
table(knn.pred,Direction.2005)

(83+43) /252

knn.pred=knn(train.X,test.X,train.Direction ,k=3)
table(knn.pred,Direction.2005)

mean(knn.pred==Direction.2005)

### An application to Caravan Insurance Data 
dim(Caravan) 
attach(Caravan)
summary(Purchase)

standardized.X=scale(Caravan [,-86]) # why scale and center expression data when include it as predictors for genomic prediction 
var ( Caravan [ ,1]) 
var ( Caravan [ ,2])
var(standardized.X[,1]) 
var(standardized.X[,2])

test =1:1000
train.X=standardized.X[-test ,]
test.X=standardized.X[test ,]
train.Y=Purchase [-test]
test.Y=Purchase [test]
set.seed (1)
knn.pred=knn(train.X,test.X,train.Y,k=1)
mean(test.Y!=knn.pred) 
mean(test.Y!="No")

table(knn.pred,test.Y) 
test.Y
knn.pred 
9/(68+9) # the fraction of individual predicted to buy insurance 

knn.pred=knn(train.X,test.X,train.Y,k=3) 
table(knn.pred,test.Y)
test.Y 
knn.pred
5/26
knn.pred=knn(train.X,test.X,train.Y,k=5)
table(knn.pred,test.Y)
test.Y 
knn.pred 
4/15

# Logestic regression  
glm.fits=glm(Purchase ~.,data=Caravan ,family=binomial, subset=-test)
glm.probs=predict(glm.fits,Caravan[test,],type="response")
glm.pred=rep("No",1000)
glm.pred[glm.probs >.5]="Yes" # need to understand these more... 
table(glm.pred,test.Y)
test.Y 
glm.pred 

glm.pred=rep("No",1000)
glm.pred[glm.probs >.25]=" Yes"
table(glm.pred,test.Y)
test.Y
glm.pred 
11/(22+11) 

### QDA handels the problem when the variance for different classes of K are different. 
### QDA handels the problem when the correlation of different P in different class of K are different   
```

### cross-validation 
```{r}
library(ISLR)

## validation set approach 
set.seed(1)
train=sample(392,196)

lm.fit=lm(mpg~horsepower ,data=Auto,subset=train)
attach(Auto)
mean((mpg~predict(lm.fit, Auto))[-train]^2) 

lm.fit2=lm(mpg~poly(horsepower ,2),data=Auto,subset=train) 
mean((mpg-predict(lm.fit2,Auto))[-train]^2)

lm.fit3=lm(mpg ~poly(horsepower ,3),data=Auto,subset=train)
mean((mpg-predict(lm.fit3,Auto))[-train]^2)

set.seed (2)
train=sample(392,196)
lm.fit=lm(mpg ~horsepower ,subset=train)

mean((mpg-predict(lm.fit,Auto))[-train]^2)
lm.fit2=lm(mpg~poly(horsepower ,2),data=Auto,subset=train) 
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
lm.fit3=lm(mpg~poly(horsepower ,3),data=Auto,subset=train) 
mean((mpg-predict(lm.fit3,Auto))[-train]^2)

glm.fit=glm(mpg~horsepower ,data=Auto) 
coef(glm.fit)

lm.fit=lm(mpg~horsepower ,data=Auto) 
coef(lm.fit)

### LOOCV 
library(boot)
glm.fit=glm(mpg~horsepower ,data=Auto)
cv.err=cv.glm(Auto,glm.fit)
cv.err$delta # why two same values 

### 10 fold cross validation 
cv.error=rep(0,5) 
for (i in 1:5){
glm.fit=glm(mpg~poly(horsepower ,i),data=Auto) # different polynomial levels 
cv.error[i]=cv.glm(Auto,glm.fit)$delta[1] } # LOOCV 
cv.error 

set.seed(17)
cv.error.10=rep(0,10) 
for (i in 1:10){
glm.fit=glm(mpg~poly(horsepower ,i),data=Auto) 
cv.error.10[i]=cv.glm(Auto,glm.fit,K=10)$delta[1] # 10 fold CV 
}
cv.error.10 

### glm(family="binomial") VS glm()
### bias-variance trade off... understand... 
```

### bootstrap 
```{r}
library(tidyverse)
library(ISLR)
library(boot)

alpha.fn=function(data,index){
X=data$X[index]
Y=data$Y[index]
return((var(Y)-cov(X,Y))/(var(X)+var(Y)-2*cov(X,Y))) } # The function then outputs the estimate for α based on the selected observations. 

alpha.fn(Portfolio ,1:100) # sample 1 to 100 
set.seed(1)
alpha.fn(Portfolio,sample(100,100,replace=T)) # draw with replacement 

Portfolio %>% dim()
boot(Portfolio ,alpha.fn,R=1000) # the boot() function automates the bootstrap process 

## Estimating the Accuracy of a Linear Regression Model

boot.fn=function(data,index){
return(coef(lm(mpg~horsepower ,data=data,subset=index))) }
boot.fn(Auto ,1:392)

Auto %>% dim()

# use boot.fn() to create bootstrap estimates for the intercept and slope terms by randomly sampling from among the observation with replacement 
set.seed(1)
boot.fn(Auto,sample(392,392,replace=T))
boot.fn(Auto,sample(392,392,replace=T))

# use the boot() function to compute the standard errors of 1,000 bootstrap estimates for the intercept and slope terms. 
boot(Auto ,boot.fn ,1000)
# This indicates that the bootstrap estimate for SE(βˆ0) is 0.86, and that the bootstrap estimate for SE(βˆ1) is 0.0074. 

summary(lm(mpg~horsepower ,data=Auto))$coef

boot.fn=function(data,index)
coefficients(lm(mpg~horsepower+I(horsepower^2),data=data,
subset=index))
set.seed(1)
boot(Auto ,boot.fn ,1000) 

summary(lm(mpg~horsepower+I(horsepower^2),data=Auto))$coef   
```

### chapter 6 
```{r}
# best subset selection 
library(tidyverse)
library(ISLR) 
names(Hitters)

dim(Hitters)
sum(is.na(Hitters$Salary))

Hitters=na.omit(Hitters) 
dim(Hitters) 
sum(is.na(Hitters))

library(leaps)
regfit.full=regsubsets(Salary~.,Hitters)
summary(regfit.full)

regfit.full=regsubsets(Salary~.,data=Hitters ,nvmax=19) 
reg.summary=summary(regfit.full)

names(reg.summary)

reg.summary$rsq
par(mfrow=c(2,2))
plot(reg.summary$rss ,xlab="Number of Variables ",ylab="RSS",
type="l")
plot(reg.summary$adjr2 ,xlab="Number of Variables ",
ylab="Adjusted RSq",type="l")

which.max(reg.summary$adjr2)
points(11,reg.summary$adjr2[11], col="red",cex=2,pch=20)

plot(reg.summary$cp ,xlab="Number of Variables ",ylab="Cp", type='l') # what is Cp??? 
which.min(reg.summary$cp ) # [1] 10
points(10,reg.summary$cp [10],col="red",cex=2,pch=20)
which.min(reg.summary$bic )
plot(reg.summary$bic ,xlab="Number of Variables ",ylab="BIC", type='l')
points(6,reg.summary$bic [6],col="red",cex=2,pch=20)

plot(regfit.full,scale="r2")
plot(regfit.full,scale="adjr2") 
plot(regfit.full,scale="Cp")
plot(regfit.full,scale="bic")

coef(regfit.full ,6)

# forward and backward stepwise selection 
regfit.fwd=regsubsets(Salary~.,data=Hitters,nvmax=19, method ="forward")
summary(regfit.fwd)
regfit.bwd=regsubsets(Salary~.,data=Hitters,nvmax=19, method ="backward") 
summary(regfit.bwd)  

coef(regfit.full ,7)
coef(regfit.fwd ,7)
coef(regfit.bwd ,7)  

par(mfrow=c(3,1))
plot(regfit.full,scale="bic")
plot(regfit.bwd,scale="bic")
plot(regfit.fwd,scale="bic")
```

### test error & ridge regression 
```{r}
### choosing among models using the validation set approach and cross-validataion 
set.seed(1)
library(ISLR)
library(tidyverse)
library(leaps)

data(Hitters)
train=sample(c(TRUE,FALSE), nrow(Hitters),rep=TRUE)
test =(! train )

train %>% length()
dim(Hitters)
length(test)

regfit.best=regsubsets(Salary~.,data=Hitters[train,], nvmax =19) # best subset using only traning data 
colnames(Hitters)
test.mat<-model.matrix(Salary~.,data=Hitters[test,]) 
sum(test)
test.mat %>% dim() # 121 20, why 121?   
test2 <- Hitters[test,] # 148 20 
sum(!(rownames(test2) %in% rownames(test.mat))) 
148-121 # why ??? 
val.errors=rep(NA,19) 

for(i in 1:19){
coefi=coef(regfit.best,id=i)
pred=test.mat[,names(coefi)]%*%coefi
val.errors[i]=mean((Hitters$Salary[test]-pred)^2) }

val.errors
which.min(val.errors)
coef(regfit.best ,10)

predict.regsubsets =function (object ,newdata ,id ,...){
form=as.formula(object$call [[2]])
mat=model.matrix(form,newdata)
coefi=coef(object ,id=id)
xvars=names(coefi)
mat[,xvars]%*%coefi } ### don't understand 

regfit.best=regsubsets(Salary~.,data=Hitters ,nvmax=19)
coef(regfit.best ,10)

k=10
set.seed(1)
folds=sample(1:k,nrow(Hitters),replace=TRUE)
folds
nrow(Hitters)

cv.errors=matrix(NA,k,19, dimnames=list(NULL, paste(1:19))) # row and colnames 
cv.errors %>% dim() 
?matrix 
list(NULL, paste(1:19))
cv.errors %>% colnames()

for(j in 1:k){
best.fit = regsubsets(Salary~., data = Hitters[folds!=j,], nvmax = 19)
for (i in 1:19){
pred=predict(best.fit,Hitters[folds==j,],id=i) 
cv.errors[j,i]=mean( (Hitters$Salary[folds==j]-pred)^2)
}
} ### doesn't work... 

mean.cv.errors=apply(cv.errors ,2,mean) 
mean.cv.errors
par(mfrow=c(1,1))
plot(mean.cv.errors ,type='b')

reg.best=regsubsets (Salary~.,data=Hitters , nvmax=19)
coef(reg.best ,11)

### ridge regression 
x=model.matrix(Salary~.,Hitters)[,-1] 
x %>% dim()  # 263... why !!!! 
dim(Hitters) # why some rows were missing? 
y=Hitters$Salary
y %>% length()

library(glmnet)
grid=10^seq(10,-2,length=100)
grid 
seq(10,-2,length=100) 
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)

dim(coef(ridge.mod))
ridge.mod$lambda [50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
ridge.mod$lambda [60]
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
predict(ridge.mod,s=50,type="coefficients")[1:20,]
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]

ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh =1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,]) 
mean((ridge.pred-y.test)^2)

mean((mean(y[train])-y.test)^2)
ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])
mean((ridge.pred-y.test)^2) 
```

### PCA, dimension reduction 
```{r}
library(tidyverse)
data(Hitters)
library(pls)
library(ISLR) 
dim(Hitters) # 322 20 

# use on all data 
set.seed(2)
pcr.fit=pcr(Salary~., data=Hitters ,scale=TRUE, validation ="CV")
summary(pcr.fit)

validationplot(pcr.fit,val.type="MSEP")

### training test data 
set.seed(1)
train=sample(c(TRUE,FALSE), nrow(Hitters),rep=TRUE)
test =(!train)
pcr.fit=pcr(Salary~., data=Hitters,subset=train,scale=TRUE, validation ="CV")
summary(pcr.fit)
validationplot(pcr.fit,val.type="MSEP")

# check test data 
?predict
pcr.pred=predict(pcr.fit, Hitters[test,], ncomp=7) 
mean((pcr.pred-Hitters[test,]$Salary)^2) ### doesn't work, due to NA 

# decided the best ncomp & use it on all data 
pcr.fit=pcr(Salary~., data=Hitters,scale=TRUE,ncomp=7)
summary(pcr.fit) 

### PLS 
set.seed(1)
pls.fit=plsr(Salary~.,data = Hitters, subset=train, scale = T, validation ="CV")
summary(pls.fit)

validationplot(pls.fit,val.type="MSEP")

### need to check 
# pls.pred=predict(pls.fit,Hitters[test,],ncomp=2) 
# mean((pls.pred-Hitters[test,]$Salary)^2) 

pls.fit=plsr(Salary~., data=Hitters ,scale=TRUE,ncomp=2) 
summary(pls.fit) 

### note: 
# whenever doing PCA, always scale no matter for prediction or explanatory purpose, eg. PCA on 
```

### non-linear modeling 
```{r}
library(ISLR) 
library(tidyverse)

# many of the complex non-linear fitting procedures discussed can be easily implemented in R.  

# polymomial regression and step function 
fit=lm(wage~poly(age,4),data=Wage) # 
coef(summary(fit))

fit2=lm(wage~poly(age,4,raw=T),data=Wage) # raw	if true, use raw and not orthogonal polynomials.
?poly
coef(summary(fit2))

fit2a=lm(wage~age+I(age^2)+I(age^3)+I(age^4),data=Wage)
coef(fit2a)

fit2b=lm(wage~cbind(age,age^2,age^3,age^4),data=Wage)

agelims=range(Wage$age)
age.grid=seq(from=agelims[1],to=agelims[2])
preds=predict(fit,newdata=list(age=age.grid),se=TRUE) # predict
se.bands=cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit ) # predict +/- 2se
head(se.bands) 

par(mfrow=c(1,2),mar=c(4.5,4.5,1,1) ,oma=c(0,0,4,0)) # set plot parameter
plot(Wage$age,Wage$wage,xlim=agelims ,cex=.5,col="darkgrey") # plot the value
title("Degree -4 Polynomial ",outer=T) # title 
lines(age.grid,preds$fit,lwd=2,col="blue") # add fit line 
matlines(age.grid,se.bands,lwd=1,col="blue",lty=3) # add CI line 

preds2=predict(fit2,newdata=list(age=age.grid),se=TRUE) # fit2 does not do orthogonal polynomials 
max(abs(preds$fit -preds2$fit )) # no difference in the two fit values 

fit.1=lm(wage~age,data=Wage)
fit.2=lm(wage~poly(age,2),data=Wage) 
fit.3=lm(wage~poly(age,3),data=Wage) 
fit.4=lm(wage~poly(age,4),data=Wage) 
fit.5=lm(wage~poly(age,5),data=Wage) 
anova(fit.1,fit.2,fit.3,fit.4,fit.5)

coef(summary(fit.5))     

fit.1=lm(wage~education+age,data=Wage)
fit.2=lm(wage~education+poly(age,2),data=Wage)
fit.3=lm(wage~education+poly(age,3),data=Wage)
anova(fit.1,fit.2,fit.3)

# binomial using glm, logistic regression 
fit=glm(I(wage>250)~poly(age,4),data=Wage,family=binomial)
preds=predict(fit,newdata=list(age=age.grid),se=T)

# confidence interval, predict +/- 2se 
pfit=exp(preds$fit )/(1+exp(preds$fit ))
se.bands.logit = cbind(preds$fit+2*preds$se.fit, preds$fit-2*preds$se.fit)
se.bands = exp(se.bands.logit)/(1+exp(se.bands.logit))

preds=predict(fit,newdata=list(age=age.grid),type="response", se=T)

# plot 
plot(Wage$age,I(Wage$wage>250),xlim=agelims ,type="n",ylim=c(0,.2))
points(jitter(Wage$age), I((Wage$wage>250)/5),cex=.5,pch="|",col =" darkgrey ")
lines(age.grid,pfit,lwd=2, col="blue")
matlines(age.grid,se.bands,lwd=1,col="blue",lty=3)

# fit step function 
table(cut(age,4)) # cut pick cutpoint automatically or specify cut point using breaks 
fit=lm(wage~cut(age ,4),data=Wage) 
coef(summary(fit)) 

## splines 
library(splines)
fit=lm(wage~bs(age,knots=c(25,40,60)),data=Wage) # specify knots at 25, 40, and 60 
pred=predict(fit,newdata=list(age=age.grid),se=T)

plot(Wage$age,Wage$wage,col="gray")
lines(age.grid,pred$fit,lwd=2)
lines(age.grid,pred$fit+2*pred$se ,lty="dashed")
lines(age.grid,pred$fit-2*pred$se ,lty="dashed")

# use degree of freedom option to decide knots 
dim(bs(Wage$age,knots=c(25,40,60))) # [1] 3000 6
dim(bs(Wage$age,df=6))
# [1] 3000 6
attr(bs(Wage$age,df=6),"knots")

# natural spline 
fit2=lm(wage~ns(age,df=4),data=Wage)
pred2=predict(fit2,newdata=list(age=age.grid),se=T) 
lines(age.grid, pred2$fit,col="red",lwd=2)

# smooth spline 
plot(Wage$age,Wage$wage,xlim=agelims ,cex=.5,col="darkgrey")
title (" Smoothing Spline ")
fit=smooth.spline(Wage$age,Wage$wage,df=16)
fit2=smooth.spline(Wage$age,Wage$wage,cv=TRUE)
fit2$df
lines(fit,col="red",lwd=2) # smooth spline with df of 16 
lines(fit2,col="blue",lwd=2)
legend("topright",legend=c("16 DF","6.8 DF"), 
col=c("red","blue"),lty=1,lwd=2,cex=.8) # add legend 

# local regression, loess(), to be continued... 
plot(Wage$age,Wage$wage,xlim=agelims ,cex=.5,col="darkgrey")
title (" Local Regression ")
fit=loess(wage~age,span=.2,data=Wage)
fit2=loess(wage~age,span=.5,data=Wage)
lines(age.grid,predict(fit,data.frame(age=age.grid)), col="red",lwd=2)
lines(age.grid,predict(fit2,data.frame(age=age.grid)), col="blue",lwd=2)
legend("topright",legend=c("Span=0.2","Span=0.5"), col=c("red","blue"),lty=1,lwd=2,cex=.8) 

# GAM 
gam1=lm(wage~ns(year ,4)+ns(age ,5)+education ,data=Wage) # 4 & 5 are degree of freedom

library("gam")
gam.m3=gam(wage~s(year ,4)+s(age ,5)+education ,data=Wage)

par(mfrow=c(1,3))
plot(gam.m3, se=TRUE,col="blue")

plot.gam(gam1, se=TRUE, col="red")

gam.m1=gam(wage~s(age ,5)+education ,data=Wage) # model that exclude year 
gam.m2=gam(wage~year+s(age ,5)+education ,data=Wage) # model that use linear function  
anova(gam.m1,gam.m2,gam.m3,test="F") 
summary(gam.m3)

preds=predict(gam.m2,newdata=Wage)

gam.lo=gam(wage~s(year,df=4)+lo(age,span=0.7)+education, data=Wage)
plot.gam(gam.lo, se=TRUE, col="green")

gam.lo.i=gam(wage~lo(year,age,span=0.5)+education,data=Wage)

library(akima)
plot(gam.lo.i)

gam.lr=gam(I(wage>250)~year+s(age,df=5)+education, family=binomial,data=Wage)
par(mfrow=c(1,3))
plot(gam.lr,se=T,col="green")

table(Wage$education ,I(Wage$wage >250))

gam.lr.s=gam(I(wage>250)~year+s(age,df=5)+education,family= binomial,data=Wage,subset=(education!="1. < HS Grad"))
plot(gam.lr.s,se=T,col="green") 

```


### decision trees 
```{r}
library(tree)
 
library(ISLR)
attach(Carseats)
High=ifelse(Sales <=8,"No","Yes")

Carseats =data.frame(Carseats ,High)
tree.carseats =tree(High~.-Sales ,Carseats )

summary(tree.carseats)

plot(tree.carseats )
text(tree.carseats ,pretty =0)

tree.carseats 

set.seed (2)
train=sample(1:nrow(Carseats), 200)
Carseats.test=Carseats [-train ,]
High.test=High[-train]
tree.carseats=tree(High~.-Sales,Carseats,subset=train)
tree.pred=predict(tree.carseats,Carseats.test,type="class")
table(tree.pred ,High.test)

(86+57) /200

set.seed (3)
cv.carseats =cv.tree(tree.carseats ,FUN=prune.misclass )
names(cv.carseats )
cv.carseats

par(mfrow=c(1,2))
plot(cv.carseats$size ,cv.carseats$dev ,type="b")
plot(cv.carseats$k ,cv.carseats$dev ,type="b") 

par(mfrow=c(1,1))
prune.carseats=prune.misclass(tree.carseats,best=9)
plot(prune.carseats )
text(prune.carseats,pretty=0)

tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred ,High.test)
(94+60) /200


prune.carseats=prune.misclass(tree.carseats,best=15)
plot(prune.carseats )
text(prune.carseats,pretty=0)
tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred ,High.test)
(86+62) /200

# regression tree
library(MASS)
set.seed (1)
train = sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston=tree(medv~.,Boston ,subset=train)
summary(tree.boston)

plot(tree.boston)
text(tree.boston ,pretty=0)

cv.boston=cv.tree(tree.boston)
plot(cv.boston$size ,cv.boston$dev ,type='b')
plot(cv.boston$size ,cv.boston$k ,type='b')

prune.boston=prune.tree(tree.boston ,best=5) # why choose 5 
plot(prune.boston)
text(prune.boston ,pretty=0)

yhat=predict(tree.boston ,newdata=Boston[-train ,])
boston.test=Boston[-train ,"medv"]
plot(yhat,boston.test)
abline (0 ,1) # intecept of 0 & slope of 1 
mean((yhat-boston.test)^2)

# bagging, random forest, and boosting 
library(randomForest)
set.seed (1)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,importance =TRUE)
bag.boston

yhat.bag = predict(bag.boston ,newdata=Boston[-train ,])
plot(yhat.bag, boston.test)
abline (0 ,1)
mean((yhat.bag-boston.test)^2)

bag.boston=randomForest(medv~.,data=Boston,subset=train, mtry=13,ntree=25)
yhat.bag = predict(bag.boston ,newdata=Boston[-train ,])
mean((yhat.bag-boston.test)^2)

set.seed (1)
rf.boston=randomForest(medv~.,data=Boston,subset=train,mtry=6,importance =TRUE)
yhat.rf = predict(rf.boston ,newdata=Boston[-train ,])
mean((yhat.rf-boston.test)^2)

importance(rf.boston)
varImpPlot(rf.boston)

library(gbm)
set.seed(1)
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=5000, interaction.depth=4)

summary(boost.boston)
boost.boston

par(mfrow=c(1,2)) 
plot(boost.boston ,i="rm") 
plot(boost.boston ,i="lstat")

yhat.boost=predict(boost.boston,newdata=Boston[-train,], n.trees=5000)
mean((yhat.boost -boston.test)^2)

boost.boston=gbm(medv~.,data=Boston[train,],distribution= "gaussian",n.trees=5000, interaction.depth=4,shrinkage =0.2, verbose =F)
yhat.boost=predict(boost.boston,newdata=Boston[-train,], n.trees=5000)
mean((yhat.boost -boston.test)^2) 
```















